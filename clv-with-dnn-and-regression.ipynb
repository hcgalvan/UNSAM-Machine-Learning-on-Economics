{"cells":[{"cell_type":"markdown","metadata":{},"source":["The DNN approach is suitable for project with a huge amount of data. Since both DDN approach and statistical approach with BGF and Gamma  (Lifetimes) perform with merely same results , we only need to choose which one suits us for the interface we are going to build. \n","\n","(I thought of a section in the interface which could be a radio button or a dropdown to choose between Statistical approach using Lifetimes (BGF and Gamma) versus DNN and compare the two but the blog says the results are merely the same and since we are not going to integrate Lifetimes we are just opting for DNN and Regression"]},{"cell_type":"markdown","metadata":{},"source":["### Necessary libraries to import"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[],"source":["# pip install pandas-profiling[notebook]"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[],"source":["# from pandas_profiling import ProfileReport"]},{"cell_type":"markdown","metadata":{},"source":["# ML vs Statistical Approach for CLV"]},{"cell_type":"code","execution_count":50,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from datetime import datetime\n","\n","#Statistical LTV\n","from lifetimes import BetaGeoFitter, GammaGammaFitter\n","from lifetimes.utils import calibration_and_holdout_data, summary_data_from_transaction_data\n","\n","# ML approach to LTV\n","import tensorflow as tf \n","#import tensorflow_probability as tfp\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_docs as tfdocs\n","import tensorflow_docs.modeling as tfmodel\n","import tensorflow_docs.plots\n","\n","\n","# evaluation\n","from sklearn.metrics import r2_score\n","from sklearn.metrics  import mean_absolute_error\n","\n","# Plotting \n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.use('TkAgg')\n","import seaborn as sns \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data preprocessing and modeling (feature engineering)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Make the default figures a bit bigger\n","plt.rcParams['figure.figsize'] = (10,7) "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Read the dataset \n","retail_ol_h1 = pd.read_csv('./data/Year 2009-2010_train.csv',  encoding= 'unicode_escape')\n","retail_ol_h2 = pd.read_csv('./data/Year 2010-2011_train.csv',  encoding= 'unicode_escape')\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["frames = [retail_ol_h1, retail_ol_h2]\n","results = pd.concat(frames)\n","data = results.copy()\n","data['InvoiceDate'] = pd.to_datetime(data.InvoiceDate, format = '%Y/%m/%d %H:%M')"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["\n","#Datetime transformation\n","data['date'] = pd.to_datetime(data.InvoiceDate.dt.date)\n","data['time'] = data.InvoiceDate.dt.time\n","data['hour'] = data['time'].apply(lambda x: x.hour)\n","data['weekend'] = data['date'].apply(lambda x: x.weekday() in [5, 6])\n","data['dayofweek'] = data['date'].apply(lambda x: x.dayofweek)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["data.drop(['Unnamed: 0'], axis =1, inplace=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["       Invoice StockCode                         Description  Quantity  \\\n","149492  517978     22634      CHILDS BREAKFAST SET SPACEBOY          2   \n","265418  501929     20738            GREEN MINI TAPE MEASURE         10   \n","252278  496290    84750A         PINK SMALL GLASS CAKE STAND         8   \n","29090   532659     22158  3 HEARTS HANGING DECORATION RUSTIC         5   \n","45207   576463     23356               LOVE HOT WATER BOTTLE         1   \n","\n","               InvoiceDate  Price  Customer ID         Country       date  \\\n","149492 2010-08-03 14:38:00   9.95      16316.0  United Kingdom 2010-08-03   \n","265418 2010-03-22 11:21:00   0.85      17760.0  United Kingdom 2010-03-22   \n","252278 2010-01-29 17:55:00   1.95          NaN  United Kingdom 2010-01-29   \n","29090  2010-11-14 11:18:00   2.95      16121.0  United Kingdom 2010-11-14   \n","45207  2011-11-15 11:37:00   5.95      17974.0  United Kingdom 2011-11-15   \n","\n","            time  hour  weekend  dayofweek  \n","149492  14:38:00    14    False          1  \n","265418  11:21:00    11    False          0  \n","252278  17:55:00    17    False          4  \n","29090   11:18:00    11     True          6  \n","45207   11:37:00    11    False          1  \n"]}],"source":["\n","print(data.sample(5))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["738 days 00:00:00\n"]}],"source":["#Plots a timeseries of total sales\n","data.groupby('date')['Quantity'].sum().plot()\n","#Prints the total number of days between start and end\n","print(data['date'].max() - data['date'].min())"]},{"cell_type":"markdown","metadata":{},"source":["So, I have around 1 year of data. Because the ML approach requires time periods for feature creation, training targets, and validation targets, I'll split it into the following segments:\n","1. Training Features Period - from 2011-01-01 until 2011-06-11\n","2. Training Target Period - from 2011-06-12 until 2011-09-09\n","3. Testing Features Period - from 2011-04-02 until 2011-09-10\n","4. Testing Target Period - from 2011-09-11 until 2011-12-09"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Number of Purchases: 853896\n","Total Number of transactions: 50850\n","Total Unique Days: 604\n","Total Unique Customers: 5910\n","We are predicting 89 days\n"]}],"source":["#Dataset info\n","print(f'Total Number of Purchases: {data.shape[0]}')\n","print(f'Total Number of transactions: {data.Invoice.nunique()}')\n","print(f'Total Unique Days: {data.date.nunique()}')\n","print(f\"Total Unique Customers: {data['Customer ID'].nunique()}\")\n","print(f\"We are predicting {(data['date'].max() - datetime(2011, 9, 11)).days} days\")"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline: BG/NBD + Gamma-Gamma model\n","\n","BG/NDB and Gamma-Gamma models are statistical models that model the purchasing behaviour (transactions and the average order value) by fitting different types of distributions. This type of modelling requires the data on transaction level, so let's first aggregate the dataset by the invoice. I'll be aggregating the transactional revenue which is simply calculated as the quantity times the item price."]},{"cell_type":"markdown","metadata":{},"source":["### Data Prep"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#Get revenue column\n","data['Revenue'] = data['Quantity'] * data['Price']\n","\n","#Context data for the revenue (date & customerID)\n","id_lookup = data[['Customer ID', 'Invoice', 'date']].drop_duplicates()\n","id_lookup.index = id_lookup['Invoice']\n","id_lookup = id_lookup.drop('Invoice', axis=1)\n","\n","transactions_data = pd.DataFrame(data.groupby('Invoice')['Revenue'].sum()).join(id_lookup)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Revenue</th>\n","      <th>Customer ID</th>\n","      <th>date</th>\n","    </tr>\n","    <tr>\n","      <th>Invoice</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>489434</th>\n","      <td>364.90</td>\n","      <td>13085.0</td>\n","      <td>2009-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>489435</th>\n","      <td>145.80</td>\n","      <td>13085.0</td>\n","      <td>2009-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>489436</th>\n","      <td>436.60</td>\n","      <td>13078.0</td>\n","      <td>2009-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>489437</th>\n","      <td>310.75</td>\n","      <td>15362.0</td>\n","      <td>2009-12-01</td>\n","    </tr>\n","    <tr>\n","      <th>489438</th>\n","      <td>2152.40</td>\n","      <td>18102.0</td>\n","      <td>2009-12-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Revenue  Customer ID       date\n","Invoice                                 \n","489434    364.90      13085.0 2009-12-01\n","489435    145.80      13085.0 2009-12-01\n","489436    436.60      13078.0 2009-12-01\n","489437    310.75      15362.0 2009-12-01\n","489438   2152.40      18102.0 2009-12-01"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["transactions_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["`lifetimes` package has a utility function to split the data and aggregate the features into RFM format. So here, I'm going to use it to save some time, but if the data is large, it makes sense to do this yourselve."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#Spit into train - test\n","rfm_train_test = calibration_and_holdout_data(transactions_data, 'Customer ID', 'date',\n","                                        calibration_period_end='2011-09-10',\n","                                        monetary_value_col = 'Revenue')   \n","\n","#Selecting only customers with positive value in the calibration period (otherwise Gamma-Gamma model doesn't work)\n","rfm_train_test = rfm_train_test.loc[rfm_train_test['monetary_value_cal'] > 0, :]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3608, 7)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>frequency_cal</th>\n","      <th>recency_cal</th>\n","      <th>T_cal</th>\n","      <th>monetary_value_cal</th>\n","      <th>frequency_holdout</th>\n","      <th>monetary_value_holdout</th>\n","      <th>duration_holdout</th>\n","    </tr>\n","    <tr>\n","      <th>Customer ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12346.0</th>\n","      <td>9.0</td>\n","      <td>400.0</td>\n","      <td>635.0</td>\n","      <td>8569.270000</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>90.0</td>\n","    </tr>\n","    <tr>\n","      <th>12347.0</th>\n","      <td>5.0</td>\n","      <td>275.0</td>\n","      <td>314.0</td>\n","      <td>567.698000</td>\n","      <td>2.0</td>\n","      <td>662.34</td>\n","      <td>90.0</td>\n","    </tr>\n","    <tr>\n","      <th>12348.0</th>\n","      <td>3.0</td>\n","      <td>190.0</td>\n","      <td>348.0</td>\n","      <td>439.693333</td>\n","      <td>1.0</td>\n","      <td>160.00</td>\n","      <td>90.0</td>\n","    </tr>\n","    <tr>\n","      <th>12349.0</th>\n","      <td>3.0</td>\n","      <td>328.0</td>\n","      <td>645.0</td>\n","      <td>777.383333</td>\n","      <td>1.0</td>\n","      <td>1326.49</td>\n","      <td>90.0</td>\n","    </tr>\n","    <tr>\n","      <th>12352.0</th>\n","      <td>5.0</td>\n","      <td>130.0</td>\n","      <td>302.0</td>\n","      <td>141.806000</td>\n","      <td>3.0</td>\n","      <td>263.76</td>\n","      <td>90.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             frequency_cal  recency_cal  T_cal  monetary_value_cal  \\\n","Customer ID                                                          \n","12346.0                9.0        400.0  635.0         8569.270000   \n","12347.0                5.0        275.0  314.0          567.698000   \n","12348.0                3.0        190.0  348.0          439.693333   \n","12349.0                3.0        328.0  645.0          777.383333   \n","12352.0                5.0        130.0  302.0          141.806000   \n","\n","             frequency_holdout  monetary_value_holdout  duration_holdout  \n","Customer ID                                                               \n","12346.0                    0.0                    0.00              90.0  \n","12347.0                    2.0                  662.34              90.0  \n","12348.0                    1.0                  160.00              90.0  \n","12349.0                    1.0                 1326.49              90.0  \n","12352.0                    3.0                  263.76              90.0  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["print(rfm_train_test.shape)\n","rfm_train_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Modelling"]},{"cell_type":"markdown","metadata":{},"source":["Now, we can use the RFM calibration data to train the models."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["<lifetimes.BetaGeoFitter: fitted with 3608 subjects, a: 0.06, alpha: 54.11, b: 0.43, r: 0.95>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["#Train the BG/NBD model\n","bgf = BetaGeoFitter(penalizer_coef=0.1)\n","bgf.fit(rfm_train_test['frequency_cal'], rfm_train_test['recency_cal'], rfm_train_test['T_cal'])"]},{"cell_type":"markdown","metadata":{},"source":["To fit Gamma-Gamma model, we first need to make sure that the monetary value and frequency are not correlated as this is one of the basi assumptions of the model."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>monetary_value_cal</th>\n","      <th>frequency_cal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>monetary_value_cal</th>\n","      <td>1.000000</td>\n","      <td>0.170918</td>\n","    </tr>\n","    <tr>\n","      <th>frequency_cal</th>\n","      <td>0.170918</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    monetary_value_cal  frequency_cal\n","monetary_value_cal            1.000000       0.170918\n","frequency_cal                 0.170918       1.000000"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#Train Gamma-Gamma\n","rfm_train_test[['monetary_value_cal', 'frequency_cal']].corr()"]},{"cell_type":"markdown","metadata":{},"source":["They are not, so we can continue with fitting"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["<lifetimes.GammaGammaFitter: fitted with 3608 subjects, p: 1.39, q: 3.80, v: 553.14>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["ggf = GammaGammaFitter(penalizer_coef = 0)\n","ggf.fit(rfm_train_test['frequency_cal'],\n","        rfm_train_test['monetary_value_cal'])"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction"]},{"cell_type":"markdown","metadata":{},"source":["Prediction is done in three steps:\n","1. Predict the expected number of transactions\n","2. Predict the average order value\n","3. Multiply number of transations by the average order value"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#Predict the expected number of transactions in the next 89 days\n","predicted_bgf = bgf.predict(89,\n","                        rfm_train_test['frequency_cal'], \n","                        rfm_train_test['recency_cal'], \n","                        rfm_train_test['T_cal'])\n","trans_pred = predicted_bgf.fillna(0)\n","\n","#Predict the average order value\n","monetary_pred = ggf.conditional_expected_average_profit(rfm_train_test['frequency_cal'],\n","                                        rfm_train_test['monetary_value_cal'])\n","\n","#Putting it all together\n","sales_pred = trans_pred * monetary_pred"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["actual = rfm_train_test['monetary_value_holdout'] *  rfm_train_test['frequency_holdout']"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def evaluate(actual, sales_prediction):\n","    print(f\"Total Sales Actual: {np.round(actual.sum())}\")\n","    print(f\"Total Sales Predicted: {np.round(sales_prediction.sum())}\")\n","    print(f\"Individual R2 score: {r2_score(actual, sales_prediction)} \")\n","    print(f\"Individual Mean Absolute Error: {mean_absolute_error(actual, sales_prediction)}\")\n","    plt.scatter(sales_prediction, actual)\n","    plt.xlabel('Prediction')\n","    plt.ylabel('Actual')      \n","    plt.show()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Sales Actual: 1644057.0\n","Total Sales Predicted: 1393624.0\n","Individual R2 score: 0.6680390533741762 \n","Individual Mean Absolute Error: 313.54211073164504\n"]}],"source":["\n","evaluate(actual, sales_pred)"]},{"cell_type":"markdown","metadata":{},"source":["It seems like the model does a fairly good job at predicting the customer's revenue for the next 3 months. Let's take a look if we can get similar performance from the Machine Learning approach.b"]},{"cell_type":"markdown","metadata":{},"source":["## ML Approach"]},{"cell_type":"markdown","metadata":{},"source":["This approach differs from statistical one, in a sense that it doesn't fit a distribution for latent parameters, but models the conditional expectations explicitly. I'm going to be using a small Deep Neural Network because it does quite a good job at implicit feature engineering and models the interaction effects quite nicely. I'm going to split my work into 3 parts:\n","1. Feature engineering for train and test periods\n","2. Modelling\n","3. Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Engineering\n","The possibility to include additional features into the model besides the RFM is the greates advantage of this approach. However, it's also the main limitation as **your model is going to be only as good as your features**. So, make sure to spend a lot of time on this section, and experiment yourselve to find the best features.  "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["#  Feature engineering \n","def get_features(data, feature_start, feature_end, target_start, target_end):\n","    \"\"\"\n","    Function that outputs the features and targets on the user-level.\n","    Inputs:\n","        * data - a dataframe with raw data\n","        * feature_start - a string start date of feature period\n","        * feature_end - a  string end date of feature period\n","        * target_start - a  string start date of target period\n","        * target_end - a  string end date of target period\n","    \"\"\"\n","    features_data = data.loc[(data.date >= feature_start) & (data.date <= feature_end), :]\n","    print(f'Using data from {(pd.to_datetime(feature_end) - pd.to_datetime(feature_start)).days} days')\n","    print(f'To predict {(pd.to_datetime(target_end) - pd.to_datetime(target_start)).days} days')\n","    \n","    #Transactions data features\n","    total_rev = features_data.groupby('Customer ID')['Revenue'].sum().rename('total_revenue')\n","    recency = (features_data.groupby('Customer ID')['date'].max() - features_data.groupby('Customer ID')['date'].min()).apply(lambda x: x.days).rename('recency')\n","    frequency = features_data.groupby('Customer ID')['InvoiceDate'].count().rename('frequency')\n","    t = features_data.groupby('Customer ID')['date'].min().apply(lambda x: (datetime(2011, 6, 11) - x).days).rename('t')\n","    time_between = (t / frequency).rename('time_between')\n","    avg_basket_value = (total_rev / frequency).rename('avg_basket_value')\n","    avg_basket_size = (features_data.groupby('Customer ID')['Quantity'].sum() / frequency).rename('avg_basket_Size')\n","    returns = features_data.loc[features_data['Revenue'] < 0, :].groupby('Customer ID')['InvoiceDate'].count().rename('num_returns')\n","    hour = features_data.groupby('Customer ID')['hour'].median().rename('purchase_hour_med')\n","    dow = features_data.groupby('Customer ID')['dayofweek'].median().rename('purchase_dow_med')\n","    weekend =  features_data.groupby('Customer ID')['weekend'].mean().rename('purchase_weekend_prop')\n","    train_data = pd.DataFrame(index = rfm_train_test.index)\n","    train_data = train_data.join([total_rev, recency, frequency, t, time_between, avg_basket_value, avg_basket_size, returns, hour, dow, weekend])\n","    train_data = train_data.fillna(0)\n","    \n","    #Target data\n","    target_data = data.loc[(data.date >= target_start) & (data.date <= target_end), :]\n","    target_quant = target_data.groupby(['Customer ID'])['date'].nunique()\n","    target_rev = target_data.groupby(['Customer ID'])['Revenue'].sum().rename('target_rev')\n","    train_data = train_data.join(target_rev).fillna(0)\n","    \n","    return train_data.iloc[:, :-1], train_data.iloc[:, -1]"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using data from 363 days\n","To predict 334 days\n"]}],"source":["X_train, y_train = get_features(data, '2010-01-01', '2010-12-30', '2010-12-31', '2011-11-30')"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using data from 363 days\n","To predict 334 days\n"]}],"source":["X_test, y_test = get_features(data, '2010-01-01', '2010-12-30', '2010-12-31', '2011-11-30')"]},{"cell_type":"markdown","metadata":{},"source":["### Modelling\n","Here, I'm going to use a Keras API to Tensorflow to build a simple DNN. Architecture here doesn't really matter because the problem is simple and small enough. However, if you have more data, make sure to fine tune the model. Start small, and see if the performance increases as the complexity increases."]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["#DNN\n","def build_model():\n","    model = keras.Sequential([\n","    layers.Dense(32, activation='relu', input_shape=[len(X_train.columns), ]),\n","    layers.Dropout(0.3),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dense(1)\n","    ])\n","\n","    optimizer = tf.keras.optimizers.Adam(0.001)\n","   \n","    model.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['mae', 'mse'])\n","    \n","    return model\n","\n"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"data":{"text/plain":["Customer ID\n","12346.0    77183.60\n","12347.0     2726.27\n","12348.0      612.68\n","12349.0     1326.49\n","12352.0     1373.01\n","             ...   \n","18280.0       95.70\n","18281.0       64.32\n","18283.0     1493.47\n","18286.0        0.00\n","18287.0     1554.32\n","Name: target_rev, Length: 3608, dtype: float64"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["y_train"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>total_revenue</th>\n","      <th>recency</th>\n","      <th>frequency</th>\n","      <th>t</th>\n","      <th>time_between</th>\n","      <th>avg_basket_value</th>\n","      <th>avg_basket_Size</th>\n","      <th>num_returns</th>\n","      <th>purchase_hour_med</th>\n","      <th>purchase_dow_med</th>\n","      <th>purchase_weekend_prop</th>\n","    </tr>\n","    <tr>\n","      <th>Customer ID</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12346.0</th>\n","      <td>-83.67</td>\n","      <td>273.0</td>\n","      <td>34.0</td>\n","      <td>523.0</td>\n","      <td>15.382353</td>\n","      <td>-2.460882</td>\n","      <td>0.558824</td>\n","      <td>11.0</td>\n","      <td>13.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12347.0</th>\n","      <td>1558.26</td>\n","      <td>37.0</td>\n","      <td>78.0</td>\n","      <td>223.0</td>\n","      <td>2.858974</td>\n","      <td>19.977692</td>\n","      <td>11.038462</td>\n","      <td>0.0</td>\n","      <td>14.0</td>\n","      <td>1.0</td>\n","      <td>0.307692</td>\n","    </tr>\n","    <tr>\n","      <th>12348.0</th>\n","      <td>1070.40</td>\n","      <td>80.0</td>\n","      <td>32.0</td>\n","      <td>257.0</td>\n","      <td>8.031250</td>\n","      <td>33.450000</td>\n","      <td>48.187500</td>\n","      <td>0.0</td>\n","      <td>14.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12349.0</th>\n","      <td>2332.15</td>\n","      <td>182.0</td>\n","      <td>85.0</td>\n","      <td>408.0</td>\n","      <td>4.800000</td>\n","      <td>27.437059</td>\n","      <td>9.552941</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12352.0</th>\n","      <td>220.65</td>\n","      <td>17.0</td>\n","      <td>12.0</td>\n","      <td>211.0</td>\n","      <td>17.583333</td>\n","      <td>18.387500</td>\n","      <td>10.583333</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18280.0</th>\n","      <td>253.75</td>\n","      <td>14.0</td>\n","      <td>19.0</td>\n","      <td>213.0</td>\n","      <td>11.210526</td>\n","      <td>13.355263</td>\n","      <td>6.157895</td>\n","      <td>2.0</td>\n","      <td>15.0</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>18281.0</th>\n","      <td>97.54</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>396.0</td>\n","      <td>56.571429</td>\n","      <td>13.934286</td>\n","      <td>9.428571</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>18283.0</th>\n","      <td>544.02</td>\n","      <td>276.0</td>\n","      <td>190.0</td>\n","      <td>477.0</td>\n","      <td>2.510526</td>\n","      <td>2.863263</td>\n","      <td>1.436842</td>\n","      <td>0.0</td>\n","      <td>13.0</td>\n","      <td>3.0</td>\n","      <td>0.289474</td>\n","    </tr>\n","    <tr>\n","      <th>18286.0</th>\n","      <td>555.13</td>\n","      <td>57.0</td>\n","      <td>42.0</td>\n","      <td>352.0</td>\n","      <td>8.380952</td>\n","      <td>13.217381</td>\n","      <td>9.285714</td>\n","      <td>3.0</td>\n","      <td>11.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>18287.0</th>\n","      <td>1751.91</td>\n","      <td>189.0</td>\n","      <td>64.0</td>\n","      <td>390.0</td>\n","      <td>6.093750</td>\n","      <td>27.373594</td>\n","      <td>17.046875</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3608 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["             total_revenue  recency  frequency      t  time_between  \\\n","Customer ID                                                           \n","12346.0             -83.67    273.0       34.0  523.0     15.382353   \n","12347.0            1558.26     37.0       78.0  223.0      2.858974   \n","12348.0            1070.40     80.0       32.0  257.0      8.031250   \n","12349.0            2332.15    182.0       85.0  408.0      4.800000   \n","12352.0             220.65     17.0       12.0  211.0     17.583333   \n","...                    ...      ...        ...    ...           ...   \n","18280.0             253.75     14.0       19.0  213.0     11.210526   \n","18281.0              97.54      0.0        7.0  396.0     56.571429   \n","18283.0             544.02    276.0      190.0  477.0      2.510526   \n","18286.0             555.13     57.0       42.0  352.0      8.380952   \n","18287.0            1751.91    189.0       64.0  390.0      6.093750   \n","\n","             avg_basket_value  avg_basket_Size  num_returns  \\\n","Customer ID                                                   \n","12346.0             -2.460882         0.558824         11.0   \n","12347.0             19.977692        11.038462          0.0   \n","12348.0             33.450000        48.187500          0.0   \n","12349.0             27.437059         9.552941          0.0   \n","12352.0             18.387500        10.583333          0.0   \n","...                       ...              ...          ...   \n","18280.0             13.355263         6.157895          2.0   \n","18281.0             13.934286         9.428571          0.0   \n","18283.0              2.863263         1.436842          0.0   \n","18286.0             13.217381         9.285714          3.0   \n","18287.0             27.373594        17.046875          0.0   \n","\n","             purchase_hour_med  purchase_dow_med  purchase_weekend_prop  \n","Customer ID                                                              \n","12346.0                   13.0               0.0               0.000000  \n","12347.0                   14.0               1.0               0.307692  \n","12348.0                   14.0               0.0               0.000000  \n","12349.0                    9.0               3.0               0.000000  \n","12352.0                   10.0               0.0               0.000000  \n","...                        ...               ...                    ...  \n","18280.0                   15.0               2.0               0.000000  \n","18281.0                   10.0               1.0               0.000000  \n","18283.0                   13.0               3.0               0.289474  \n","18286.0                   11.0               4.0               0.000000  \n","18287.0                   11.0               0.0               0.000000  \n","\n","[3608 rows x 11 columns]"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/plain":["11"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["len(X_train.columns)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["# The patience parameter is the amount of epochs to check for improvement\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n","#early_stop = keras.callbacks.EarlyStopping(monitor='val_mse', patience=50)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["model = build_model()"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Expect x to be a non-empty array or dataset.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-83-ef4afef532d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m early_history = model.fit( X_train, y_train, \n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     callbacks=[early_stop, tfmodel.EpochDots()])\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."]}],"source":["early_history = model.fit( X_train, y_train, \n","                    epochs=1000, validation_split = 0.2, verbose=0,\n","                    callbacks=[early_stop, tfmodel.EpochDots()])"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["Let's see how well the model can predict the 3 next 3 months which it has never seen before. We're going to use data from the most recent period (X_test) to make sure that our forecast is as accurate as possible."]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["#Predicting\n","dnn_preds = model.predict(X_test).ravel()"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Sales Actual: 5465561.0\n","Total Sales Predicted: -832459.0\n","Individual R2 score: -0.3061928418961304 \n","Individual Mean Absolute Error: 1749.7772905954478\n"]}],"source":["#Putting the actual and predictions into the same datarame for later comparison\n","compare_df = pd.DataFrame(index=X_test.index)\n","compare_df['dnn_preds'] = dnn_preds\n","compare_df = compare_df.join(sales_pred.rename('stat_pred')).fillna(0)\n","compare_df['actual'] = y_test\n","\n","evaluate(compare_df['actual'], compare_df['dnn_preds'])"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the model is fairly accurate with mean absolute error comparable to the BG/NBD. Total predicted sales are a bit off, but this is largely due to the significant outliers. Let's now attempt to compare the two models"]},{"cell_type":"markdown","metadata":{},"source":["## Comparison"]},{"cell_type":"markdown","metadata":{},"source":["I'll attempt to compare the performance of DNN and BG/NBD models by looking at:\n","1. How well do they fit the non-outlier distribution\n","2. How much revenue do the top 20% of CLV customers generate\n","\n","It should be noted that the datasets to train the models do differ a bit. E.g. some customer IDs had to be dropped because of their returns so the expected value was replaced by 0 in BG/NBD. Still, I'm not looking at the prediction on the user level but at the aggregate so this should not affect the evaluation. "]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["#First 98% of data\n","no_out = compare_df.loc[(compare_df['actual'] <= np.quantile(compare_df['actual'], 0.985)), :]\n","\n","sns.distplot(no_out['actual'])\n","sns.distplot(no_out['dnn_preds'])\n","plt.title('Actual vs DNN Predictions')\n","plt.show()"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["sns.distplot(no_out['actual'])\n","sns.distplot(no_out['stat_pred'])\n","plt.title('Actual vs BG/NBD Predictions')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["It looks like both models correctly model the revenue as heavily skewed with long tail. Nevertheless, DNN seems to better fit the data as it doesn't have this second spike. Let's now look at the revenue of top 20%."]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Selecting the first 722 users\n","Top 20% selected by DNN have generated 234238.0\n","Top 20% selected by BG/NBD and Gamma Gamma have generated 1493337.0\n","Thats -1259099.0 of marginal revenue\n"]}],"source":["top_n = int(np.round(compare_df.shape[0] * 0.2))\n","print(f'Selecting the first {top_n} users')\n","\n","#Selecting IDs\n","dnn_ids = compare_df['dnn_preds'].sort_values(ascending=False).index[:top_n].values\n","stat_ids = compare_df['stat_pred'].sort_values(ascending=False).index[:top_n].values\n","\n","#Filtering the data\n","eval_subset = data.loc[data.date >= '2011-09-10', :]\n","\n","#Sums\n","dnn_rev = eval_subset.loc[eval_subset['Customer ID'].isin(dnn_ids), 'Revenue'].sum() \n","stat_rev = eval_subset.loc[eval_subset['Customer ID'].isin(stat_ids), 'Revenue'].sum()\n","\n","\n","print(f'Top 20% selected by DNN have generated {np.round(dnn_rev)}')\n","print(f'Top 20% selected by BG/NBD and Gamma Gamma have generated {np.round(stat_rev)}')\n","print(f'Thats {np.round(dnn_rev - stat_rev)} of marginal revenue')"]},{"cell_type":"markdown","metadata":{},"source":["The difference is only 6,134 which is quite insignificant. Hence, both methods are able to effectively pick the top 20% of most valuable customers which is not suprising, given that we've used only the transactions data in our DNN model. What about the first 10%?"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Selecting the first 361 users\n","Top 20% selected by DNN have generated 133932.0\n","Top 20% selected by BG/NBD and Gamma Gamma have generated 1194640.0\n","Thats -1060708.0 of marginal revenue\n"]}],"source":["top_n = int(np.round(compare_df.shape[0] * 0.1))\n","print(f'Selecting the first {top_n} users')\n","\n","#Selecting IDs\n","dnn_ids = compare_df['dnn_preds'].sort_values(ascending=False).index[:top_n].values\n","stat_ids = compare_df['stat_pred'].sort_values(ascending=False).index[:top_n].values\n","\n","#Filtering the data\n","eval_subset = data.loc[data.date >= '2011-09-10', :]\n","\n","#Sums\n","dnn_rev = eval_subset.loc[eval_subset['Customer ID'].isin(dnn_ids), 'Revenue'].sum() \n","stat_rev = eval_subset.loc[eval_subset['Customer ID'].isin(stat_ids), 'Revenue'].sum()\n","\n","\n","print(f'Top 20% selected by DNN have generated {np.round(dnn_rev)}')\n","print(f'Top 20% selected by BG/NBD and Gamma Gamma have generated {np.round(stat_rev)}')\n","print(f'Thats {np.round(dnn_rev - stat_rev)} of marginal revenue')"]},{"cell_type":"markdown","metadata":{},"source":["With the first 10%, the DNN model is actually worse but also by only a small percentage (1.3%). Hence, the conclusion from this experiment and comparison is - **Given only the transactions data, both DNN's performance is similar to the BG/NBD + Gamma-Gamma approach**. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#  To change the date format into details \n","# data['date'] = pd.to_datetime(data.InvoiceDate.dt.date)\n","# data['time'] = data.InvoiceDate.dt.time\n","# data['hour'] = data['time'].apply(lambda x: x.hour)\n","# data['weekend'] = data['date'].apply(lambda x: x.weekday() in [5, 6])\n","# data['dayofweek'] = data['date'].apply(lambda x: x.dayofweek)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# #  Context data for the revenue (date & customerID)\n","# id_lookup = data['CustomerID', 'InvoiceDate'].drop.duplicates()\n","# id_lookup.index = id_lookup['InvoiceDate']\n","# id_lookup = id_lookup.drop('InvoiceDate', axis=1)\n","# transaction_data = pd.DataFrame(data.groupby('InvoiceDate')['Revenue'].sum())\n"]},{"cell_type":"markdown","metadata":{},"source":["### DATA EXPLANATIONS\n","---Data gathered is for around one Year:ML approach needs time periods for featue creation, training targets,validation targets will be split according to the following segments: \n","*   Training Features Period - from 2011-01-01 until 2011-06-11\n","*   Training Target Period - from 2011-06-12 until 2011-09-09\n","*   Testing Features Period - from 2011-04-02 until 2011-09-10\n","*   Testing Target Period - from 2011-09-11 until 2011-12-09\n"]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Number of Purchases: 853896\n","Total Number of transactions: 45660\n","Total Unique Days: 604\n","Total Unique Customers: 5910\n","We are predicting 89 days\n"]}],"source":["#Dataset info\n","print(f'Total Number of Purchases: {data.shape[0]}')\n","print(f'Total Number of transactions: {data.InvoiceDate.nunique()}')\n","print(f'Total Unique Days: {data.date.nunique()}')\n","print(f\"Total Unique Customers: {data['Customer ID'].nunique()}\")\n","print(f\"We are predicting {(data['date'].max() - datetime(2011, 9, 11)).days} days\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#DNN\n","def build_model():\n","    model = keras.Sequential([\n","    layers.Dense(256, activation='relu', input_shape=[len(X_train.columns), ]),\n","    layers.Dropout(0.3),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dense(1)\n","    ])\n","\n","    optimizer = tf.keras.optimizers.Adam(0.001)\n","\n","    model.compile(loss='mse',\n","            optimizer=optimizer,\n","            metrics=['mae', 'mse'])\n","    \n","    return model\n","\n","# The patience parameter is the amount of epochs to check for improvement\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_mse', patience=50)\n","\n","model = build_model()\n","#Should take 10 sec\n","early_history = model.fit(X_train, y_train, \n","                    epochs=1000, validation_split = 0.2, verbose=0, \n","                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Sales Actual: 5465561.0\n","Total Sales Predicted: -1676.0\n","Individual R2 score: -0.0421635794136197 \n","Individual Mean Absolute Error: 1520.956218701501\n"]}],"source":["\n","def evaluate(actual, sales_prediction):\n","    print(f\"Total Sales Actual: {np.round(actual.sum())}\")\n","    print(f\"Total Sales Predicted: {np.round(sales_prediction.sum())}\")\n","    print(f\"Individual R2 score: {r2_score(actual, sales_prediction)} \")\n","    print(f\"Individual Mean Absolute Error: {mean_absolute_error(actual, sales_prediction)}\")\n","    plt.scatter(sales_prediction, actual)\n","    plt.xlabel('Prediction')\n","    plt.ylabel('Actual')      \n","    plt.show()\n","\n","#Predicting\n","dnn_preds = model.predict(X_test).ravel()\n","\n","evaluate(y_test, dnn_preds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"22bb53b43b3afb0848b0afa27adb8f2415fb61399be88b9ea69bd0b84fe58c05"},"kernelspec":{"display_name":"Python 3.9.5 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
